{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyHa8BUB8eBB",
        "outputId": "1a7be293-cb54-416e-fe65-3619370e9aea"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCV5TSRS6W7N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
        "import logging\n",
        "import re\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"imdb_scraper.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Constants\n",
        "MOVIES_CSV = 'indian_movies_ids.csv'\n",
        "TV_SHOWS_CSV = 'indian_tv_shows_ids.csv'\n",
        "DELAY_MIN = 5  \n",
        "DELAY_MAX = 10 \n",
        "PAGE_LOAD_TIMEOUT = 30  \n",
        "LOAD_MORE_TIMEOUT = 15  \n",
        "\n",
        "def setup_driver():\n",
        "    \"\"\"Set up and return a configured Chrome WebDriver.\"\"\"\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
        "\n",
        "    # Additional options to improve stability\n",
        "    chrome_options.add_argument(\"--disable-extensions\")\n",
        "    chrome_options.add_argument(\"--disable-infobars\")\n",
        "    chrome_options.add_argument(\"--disable-notifications\")\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "    chrome_options.add_experimental_option(\"useAutomationExtension\", False)\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
        "    return driver\n",
        "\n",
        "def extract_imdb_id(href):\n",
        "    \"\"\"Safely extract IMDb ID from href.\"\"\"\n",
        "    if not href:\n",
        "        return None\n",
        "\n",
        "    # Method 1: Standard format \"/title/tt0111161/\"\n",
        "    match = re.search(r'/title/(tt\\d+)/', href)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "\n",
        "    # Method 2: Alternative format with query params \"/title/tt0111161?ref_=...\"\n",
        "    match = re.search(r'/title/(tt\\d+)', href)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "\n",
        "    # Method 3: Look for any tt followed by numbers\n",
        "    match = re.search(r'(tt\\d+)', href)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_imdb_ids(driver):\n",
        "    \"\"\"Extract IMDb IDs from the currently loaded page.\"\"\"\n",
        "    imdb_ids = []\n",
        "    processed = set()  # Track processed IDs to avoid duplicates\n",
        "\n",
        "    try:\n",
        "        # Try different methods to find items with IMDb IDs\n",
        "        links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='/title/']\")\n",
        "\n",
        "        for link in links:\n",
        "            try:\n",
        "                href = link.get_attribute(\"href\")\n",
        "                imdb_id = extract_imdb_id(href)\n",
        "\n",
        "                if imdb_id and imdb_id not in processed:\n",
        "                    imdb_ids.append(imdb_id)\n",
        "                    processed.add(imdb_id)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error processing link: {e}\")\n",
        "\n",
        "        return imdb_ids\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting IMDb IDs: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_current_ids_count(driver):\n",
        "    \"\"\"Get the current number of unique IMDb IDs on the page.\"\"\"\n",
        "    unique_ids = set()\n",
        "\n",
        "    try:\n",
        "        links = driver.find_elements(By.CSS_SELECTOR, \"a[href*='/title/']\")\n",
        "        for link in links:\n",
        "            try:\n",
        "                href = link.get_attribute(\"href\")\n",
        "                imdb_id = extract_imdb_id(href)\n",
        "                if imdb_id:\n",
        "                    unique_ids.add(imdb_id)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        return len(unique_ids)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error counting IDs: {e}\")\n",
        "        return 0\n",
        "\n",
        "def scrape_content(driver, content_type):\n",
        "    \"\"\"\n",
        "    Scrape IMDb IDs for the given content type (movie or tv).\n",
        "    Continues until no more content can be loaded.\n",
        "    \"\"\"\n",
        "    if content_type == 'movie':\n",
        "        csv_file = MOVIES_CSV\n",
        "        url = 'https://www.imdb.com/search/title/?title_type=feature&release_date=2023-09-01,&num_votes=49999,&sort=num_votes,desc,&view=simple'\n",
        "    else:  \n",
        "        csv_file = TV_SHOWS_CSV\n",
        "        url = 'https://www.imdb.com/search/title/?title_type=tv_series,tv_miniseries&release_date=2018-01-01,&num_votes=8000,&sort=num_votes,desc,&view=simple'\n",
        "\n",
        "    # Load or create dataframe for tracking progress\n",
        "    if os.path.exists(csv_file):\n",
        "        df = pd.read_csv(csv_file)\n",
        "        logging.info(f\"Found existing file {csv_file} with {len(df)} entries\")\n",
        "    else:\n",
        "        df = pd.DataFrame(columns=['imdb_id'])\n",
        "        logging.info(f\"Starting new scrape for {content_type}\")\n",
        "\n",
        "    # Start the scraping process\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        WebDriverWait(driver, PAGE_LOAD_TIMEOUT).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \".lister-list, body\"))\n",
        "        )\n",
        "\n",
        "        last_count = 0\n",
        "        no_new_content_count = 0\n",
        "        load_more_attempts = 0\n",
        "\n",
        "        while True:\n",
        "            # Extract IMDb IDs from current page\n",
        "            logging.info(f\"Extracting IMDb IDs from current page view\")\n",
        "            new_imdb_ids = extract_imdb_ids(driver)\n",
        "\n",
        "            if new_imdb_ids:\n",
        "                # Create a set of IDs we already have\n",
        "                existing_ids = set(df['imdb_id']) if not df.empty else set()\n",
        "                new_ids = [id for id in new_imdb_ids if id not in existing_ids]\n",
        "\n",
        "                if new_ids:\n",
        "                    new_df = pd.DataFrame({'imdb_id': new_ids})\n",
        "                    df = pd.concat([df, new_df], ignore_index=True)\n",
        "                    df = df.drop_duplicates(subset=['imdb_id'])\n",
        "                    df.to_csv(csv_file, index=False)\n",
        "                    logging.info(f\"Saved {len(df)} entries to {csv_file} (added {len(new_ids)} new IDs)\")\n",
        "                else:\n",
        "                    logging.info(\"No new IMDb IDs found in this batch\")\n",
        "            else:\n",
        "                logging.warning(f\"No IMDb IDs found in current view\")\n",
        "\n",
        "            # Get current count of IDs on page\n",
        "            current_count = get_current_ids_count(driver)\n",
        "            logging.info(f\"Current unique IDs on page: {current_count}\")\n",
        "\n",
        "            # Check if we've reached the end\n",
        "            if current_count == last_count:\n",
        "                no_new_content_count += 1\n",
        "                if no_new_content_count >= 3:\n",
        "                    logging.info(\"No new content after multiple attempts, reached the end\")\n",
        "                    break\n",
        "            else:\n",
        "                no_new_content_count = 0\n",
        "                last_count = current_count\n",
        "\n",
        "            # Try to load more content\n",
        "            load_more_attempts += 1\n",
        "            try:\n",
        "                # First, specifically look for the button structure from the HTML provided\n",
        "                try:\n",
        "                    # Look for the specific \"50 more\" button by its classes and text\n",
        "                    load_more_button = WebDriverWait(driver, LOAD_MORE_TIMEOUT).until(\n",
        "                        EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.ipc-btn.ipc-see-more__button\"))\n",
        "                    )\n",
        "                    # Find the button containing the text \"50 more\"\n",
        "                    buttons = driver.find_elements(By.CSS_SELECTOR, \"button.ipc-btn.ipc-see-more__button\")\n",
        "                    for button in buttons:\n",
        "                        if \"50 more\" in button.text:\n",
        "                            driver.execute_script(\"arguments[0].scrollIntoView(true);\", button)\n",
        "                            time.sleep(1)  # Give time for scroll to complete\n",
        "                            driver.execute_script(\"arguments[0].click();\", button)\n",
        "                            logging.info(\"Clicked '50 more' button\")\n",
        "                            time.sleep(5)  # Wait for content to load\n",
        "                            load_more_found = True\n",
        "                            break\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Could not find the specific '50 more' button: {e}\")\n",
        "                    load_more_found = False\n",
        "\n",
        "                # If the specific button wasn't found, try alternative selectors\n",
        "                if not load_more_found:\n",
        "                    load_more_selectors = [\n",
        "                        \"span.ipc-see-more button\",\n",
        "                        \".ipc-see-more__button\",\n",
        "                        \".single-page-see-more-button button\",\n",
        "                        \"button.ipc-btn--single-padding\",\n",
        "                        \"button:contains('50 more')\",\n",
        "                        \"button.ipc-see-more__button\"\n",
        "                    ]\n",
        "\n",
        "                    for selector in load_more_selectors:\n",
        "                        try:\n",
        "                            load_more = WebDriverWait(driver, LOAD_MORE_TIMEOUT).until(\n",
        "                                EC.element_to_be_clickable((By.CSS_SELECTOR, selector))\n",
        "                            )\n",
        "                            driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more)\n",
        "                            time.sleep(1)  # Give time for scroll to complete\n",
        "                            driver.execute_script(\"arguments[0].click();\", load_more)\n",
        "                            load_more_found = True\n",
        "                            logging.info(f\"Clicked 'Load more' button using selector: {selector}\")\n",
        "\n",
        "                            # Wait for new content to load\n",
        "                            time.sleep(5)\n",
        "                            break\n",
        "                        except Exception as e:\n",
        "                            continue\n",
        "\n",
        "                if not load_more_found:\n",
        "                    logging.info(\"Could not find 'Load more' button, trying JavaScript scroll\")\n",
        "                    # Try scrolling to bottom to trigger lazy loading\n",
        "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "                    time.sleep(5)\n",
        "\n",
        "                    # Check if more content loaded after scroll\n",
        "                    pre_scroll_count = current_count\n",
        "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "                    time.sleep(5)\n",
        "                    post_scroll_count = get_current_ids_count(driver)\n",
        "\n",
        "                    if post_scroll_count <= pre_scroll_count:\n",
        "                        logging.info(\"No more content loaded after scrolling, attempting one more aggressive scroll\")\n",
        "                        # Try one more aggressive scroll\n",
        "                        for _ in range(5):\n",
        "                            driver.execute_script(\"window.scrollBy(0, 1000);\")\n",
        "                            time.sleep(1)\n",
        "\n",
        "                        final_count = get_current_ids_count(driver)\n",
        "                        if final_count <= post_scroll_count:\n",
        "                            logging.info(\"No more content loaded after aggressive scrolling, stopping\")\n",
        "                            break\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error attempting to load more content: {e}\")\n",
        "                if no_new_content_count >= 2:\n",
        "                    logging.info(\"Multiple failures to load new content, stopping\")\n",
        "                    break\n",
        "            # Random delay between attempts\n",
        "            delay = random.uniform(DELAY_MIN, DELAY_MAX)\n",
        "            logging.info(f\"Waiting {delay:.2f} seconds before next extraction\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during content scraping: {e}\")\n",
        "\n",
        "    return len(df)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the scraper.\"\"\"\n",
        "    logging.info(\"Starting IMDb scraper for Indian content\")\n",
        "\n",
        "    try:\n",
        "        # Set up the WebDriver\n",
        "        driver = setup_driver()\n",
        "\n",
        "        try:\n",
        "            # Scrape movies\n",
        "            logging.info(\"Starting to scrape Indian movies\")\n",
        "            movies_scraped = scrape_content(driver, 'movie')\n",
        "\n",
        "            # # Scrape TV shows\n",
        "            # logging.info(\"Starting to scrape Indian TV shows\")\n",
        "            # tv_scraped = scrape_content(driver, 'tv')\n",
        "\n",
        "            # Final summary\n",
        "            logging.info(f\"Completed scraping:\")\n",
        "            logging.info(f\"Movies: {movies_scraped} unique IMDb IDs\")\n",
        "            # logging.info(f\"TV Shows: {tv_scraped} unique IMDb IDs\")\n",
        "                #\n",
        "        finally:\n",
        "            # Always close the driver when done\n",
        "            driver.quit()\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unhandled exception in main: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Umx5iUpi9sVe"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compare_and_add_unique_ids(csv1_path, csv2_path, output_csv_path):\n",
        "    \"\"\"\n",
        "    Compares two CSV files based on 'imdb_id', finds unique IDs from the second CSV,\n",
        "    and adds them to a new CSV.\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the CSV files into pandas DataFrames\n",
        "    df1 = pd.read_csv(csv1_path)\n",
        "    df2 = pd.read_csv(csv2_path)\n",
        "\n",
        "    # Extract 'imdb_id' columns as sets for efficient comparison\n",
        "    ids1 = set(df1['imdb_id'])\n",
        "    ids2 = set(df2['imdb_id'])\n",
        "\n",
        "    # Find unique IDs in the second CSV that are not in the first\n",
        "    unique_ids = ids2 - ids1\n",
        "\n",
        "    # Create a new DataFrame with the unique IDs\n",
        "    unique_ids_df = pd.DataFrame({'imdb_id': list(unique_ids)})\n",
        "\n",
        "    # Save the unique IDs to a new CSV file\n",
        "    unique_ids_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "compare_and_add_unique_ids('/content/tv_datasets.csv', '/content/indian_tv_shows_ids.csv', 'uniqu_ids.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtDt1kcphEar",
        "outputId": "f9caa144-f2b8-46bf-b357-657c8d36f496"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Constants\n",
        "BASE_URL = \"https://www.imdb.com\"\n",
        "MAX_RETRIES = 3\n",
        "DELAY_BETWEEN_REQUESTS = (2, 5)  \n",
        "COLUMNS = [\n",
        "    \"imdb_id\", \"title\", \"vote_average\", \"vote_count\", \"release_date\",\n",
        "    \"original_language\", \"overview\", \"popularity\", \"genres\",\n",
        "    \"production_companies\", \"release_year\", \"cast\",\n",
        "    \"budget\", \"collection\", \"directors\"\n",
        "]\n",
        "\n",
        "def try_multiple_selectors(driver, selectors, attribute=None):\n",
        "    \"\"\"Try multiple CSS selectors and return the first matching elements\"\"\"\n",
        "    for selector in selectors:\n",
        "        try:\n",
        "            elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "            if elements:\n",
        "                if attribute:\n",
        "                    return [elem.get_attribute(attribute) for elem in elements if elem.get_attribute(attribute)]\n",
        "                else:\n",
        "                    return [elem for elem in elements if elem.text.strip()]\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    return []\n",
        "\n",
        "def get_element_text(driver, selectors, first_only=True):\n",
        "    \"\"\"Try multiple selectors and return text of first or all matching elements\"\"\"\n",
        "    elements = try_multiple_selectors(driver, selectors)\n",
        "    if not elements:\n",
        "        return \"\" if first_only else []\n",
        "\n",
        "    if first_only:\n",
        "        return elements[0].text.strip()\n",
        "    else:\n",
        "        return [elem.text.strip() for elem in elements]\n",
        "\n",
        "def get_page_with_retry(driver, url, max_retries=MAX_RETRIES):\n",
        "    \"\"\"Load a page with retry mechanism\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            driver.get(url)\n",
        "            # Wait for page to load\n",
        "            time.sleep(random.uniform(2, 4))\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "            )\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed for {url}: {e}\")\n",
        "            time.sleep(random.uniform(5, 10))  # Longer wait after failure\n",
        "    return False\n",
        "\n",
        "def extract_numeric_value(text):\n",
        "    \"\"\"Extract numeric value from text with K, M suffixes\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.strip()\n",
        "    if \"K\" in text:\n",
        "        return str(int(float(text.replace(\"K\", \"\").strip()) * 1000))\n",
        "    elif \"M\" in text:\n",
        "        return str(int(float(text.replace(\"M\", \"\").strip()) * 1000000))\n",
        "    return text.replace(\",\", \"\")\n",
        "\n",
        "def clean_money_value(value):\n",
        "    \"\"\"Clean up monetary values\"\"\"\n",
        "    if not value:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove non-essential parts like estimated or specific phrases\n",
        "    value = re.sub(r'\\(.*?\\)', '', value).strip()\n",
        "\n",
        "    # Check if it's a valid monetary value\n",
        "    if re.search(r'[₹$€£][\\d,.]+|[\\d,.]+\\s*(?:[a-zA-Z]{3}|[₹$€£])', value):\n",
        "        return value\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "def get_title_details(imdb_id, driver):\n",
        "    \"\"\"Scrape details for a specific title\"\"\"\n",
        "    details = {col: \"\" for col in COLUMNS}\n",
        "    details[\"imdb_id\"] = imdb_id\n",
        "    title_url = f\"{BASE_URL}/title/{imdb_id}/\"\n",
        "\n",
        "    success = get_page_with_retry(driver, title_url)\n",
        "    if not success:\n",
        "        return details\n",
        "\n",
        "    # Extract basic details from the current page\n",
        "    try:\n",
        "        title_selectors = [\n",
        "            'h1[data-testid=\"hero-title-block__title\"]',\n",
        "            'h1.TitleHeader__TitleText-sc-*',\n",
        "            'h1.hero__title',\n",
        "            'span[class*=\"hero-title-block__title\"]'\n",
        "        ]\n",
        "        details[\"title\"] = get_element_text(driver, title_selectors)\n",
        "\n",
        "        # If title not found, look for it in page title\n",
        "        if not details[\"title\"]:\n",
        "            try:\n",
        "                page_title = driver.title\n",
        "                title_match = re.match(r'(.+?)(?:\\s*\\([0-9]{4}\\))?\\s*-\\s*IMDb', page_title)\n",
        "                if title_match:\n",
        "                    details[\"title\"] = title_match.group(1).strip()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Rating (vote_average) - fix formatting issues\n",
        "        rating_selectors = [\n",
        "            'div[data-testid=\"hero-rating-bar__aggregate-rating__score\"] span:first-child',\n",
        "            'span[class*=\"RatingScore\"]',\n",
        "            'span.ipc-rating-star--imdb',\n",
        "            'span.AggregateRatingButton__RatingScore-*'\n",
        "        ]\n",
        "        rating = get_element_text(driver, rating_selectors)\n",
        "\n",
        "        # Clean up rating (remove \"/10\" if present)\n",
        "        if rating:\n",
        "            rating = rating.replace('/10', '').strip()\n",
        "            # Make sure it's a valid number\n",
        "            try:\n",
        "                float(rating)\n",
        "                details[\"vote_average\"] = rating\n",
        "            except:\n",
        "                details[\"vote_average\"] = \"\"\n",
        "\n",
        "        # Vote count - fix empty vote counts\n",
        "        vote_count_selectors = [\n",
        "            'div[class*=\"AggregateRatingButton__TotalRatingAmount\"]',\n",
        "            'div.sc-d541859f-3',\n",
        "            'div.imdbRating strong',\n",
        "            'span.ipc-rating-star--imdb + div',\n",
        "            'a[href*=\"ratings\"] span'\n",
        "        ]\n",
        "        vote_count = get_element_text(driver, vote_count_selectors)\n",
        "        details[\"vote_count\"] = extract_numeric_value(vote_count)\n",
        "\n",
        "        # If vote count still empty, try another approach\n",
        "        if not details[\"vote_count\"]:\n",
        "            try:\n",
        "                ratings_text = driver.find_element(By.XPATH, \"//*[contains(text(), 'votes')]\").text\n",
        "                votes_match = re.search(r'([\\d,.]+)\\s*votes', ratings_text)\n",
        "                if votes_match:\n",
        "                    details[\"vote_count\"] = votes_match.group(1).replace(',', '')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Release date\n",
        "        release_date_selectors = [\n",
        "            'li[data-testid=\"title-details-releasedate\"] .ipc-metadata-list-item__list-content-item',\n",
        "            'a[href*=\"releaseinfo\"]',\n",
        "            'span[class*=\"TitleBlockMetaData__ListItemText\"]'\n",
        "        ]\n",
        "        release_date = get_element_text(driver, release_date_selectors)\n",
        "        details[\"release_date\"] = release_date\n",
        "\n",
        "        # Release year\n",
        "        if release_date:\n",
        "            year_match = re.search(r'(\\d{4})', release_date)\n",
        "            if year_match:\n",
        "                details[\"release_year\"] = year_match.group(1)\n",
        "\n",
        "        # If year not found in release date, try other selectors\n",
        "        if not details[\"release_year\"]:\n",
        "            year_selectors = [\n",
        "                'span.TitleBlockMetaData__ListItemText-*',\n",
        "                'a[href*=\"releaseinfo\"]',\n",
        "                'span.TitleBlockMetaData__ReleaseYear-*',\n",
        "                'a.ipc-link[href*=\"releaseinfo\"]'\n",
        "            ]\n",
        "            year_text = get_element_text(driver, year_selectors)\n",
        "            if year_text:\n",
        "                year_match = re.search(r'(\\d{4})', year_text)\n",
        "                if year_match:\n",
        "                    details[\"release_year\"] = year_match.group(1)\n",
        "\n",
        "        # If still not found, check page title\n",
        "        if not details[\"release_year\"]:\n",
        "            try:\n",
        "                page_title = driver.title\n",
        "                year_match = re.search(r'\\((\\d{4})\\)', page_title)\n",
        "                if year_match:\n",
        "                    details[\"release_year\"] = year_match.group(1)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Overview/Plot\n",
        "        try:\n",
        "            plot_elem = driver.find_element(By.CSS_SELECTOR, 'span[class*=\"GenresAndPlot__TextContainerBreakpointXL\"]')\n",
        "            details[\"overview\"] = plot_elem.text.strip()\n",
        "        except NoSuchElementException:\n",
        "            # Try alternative selectors\n",
        "            try:\n",
        "                plot_elem = driver.find_element(By.CSS_SELECTOR, 'p[data-testid=\"plot\"]')\n",
        "                details[\"overview\"] = plot_elem.text.strip()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Genres\n",
        "        genre_selectors = [\n",
        "            'div[data-testid=\"genres\"] span.ipc-chip__text',\n",
        "            'a.GenresAndPlot__GenreChip-*',\n",
        "            '.ipc-chip-list--baseAlt span.ipc-chip__text',\n",
        "            'a[href*=\"genres=\"] span.ipc-chip__text'\n",
        "        ]\n",
        "\n",
        "        genres = []\n",
        "        for selector in genre_selectors:\n",
        "            try:\n",
        "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if elements:\n",
        "                    genres = [elem.text.strip() for elem in elements if elem.text.strip() and not \"more\" in elem.text.lower()]\n",
        "                    if genres:\n",
        "                        break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        details[\"genres\"] = \", \".join(genres)\n",
        "\n",
        "        # Language\n",
        "        lang_selectors = [\n",
        "            'li[data-testid=\"title-details-languages\"] .ipc-metadata-list-item__list-content-item',\n",
        "            'a[href*=\"primary_language\"]'\n",
        "        ]\n",
        "        details[\"original_language\"] = get_element_text(driver, lang_selectors)\n",
        "\n",
        "        # Production companies\n",
        "        company_selectors = [\n",
        "            'li[data-testid=\"title-details-companies\"] .ipc-metadata-list-item__list-content-item',\n",
        "            'a[href*=\"company\"]'\n",
        "        ]\n",
        "\n",
        "        companies = []\n",
        "        for selector in company_selectors:\n",
        "            try:\n",
        "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if elements:\n",
        "                    companies = [elem.text.strip() for elem in elements if elem.text.strip()]\n",
        "                    if companies:\n",
        "                        break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        details[\"production_companies\"] = \", \".join(companies)\n",
        "\n",
        "        # Cast\n",
        "        cast_selectors = [\n",
        "            'a[data-testid=\"title-cast-item__actor\"]',\n",
        "            'td.primary_photo + td a',\n",
        "            'a[data-testid=\"title-cast-item__actor\"]'\n",
        "        ]\n",
        "\n",
        "        cast = []\n",
        "        for selector in cast_selectors:\n",
        "            try:\n",
        "                elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                if elements:\n",
        "                    cast = [elem.text.strip() for elem in elements[:10] if elem.text.strip()]\n",
        "                    if cast:\n",
        "                        break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        details[\"cast\"] = \", \".join(cast)\n",
        "\n",
        "\n",
        "        def clean_money_value(value):\n",
        "          if not value:\n",
        "              return \"\"\n",
        "\n",
        "          value = value.strip()\n",
        "          value = re.sub(r'\\(.*?\\)', '', value).strip()\n",
        "          multiplier = 1\n",
        "          currency = \"\"\n",
        "\n",
        "          # Check and extract currency\n",
        "          if value.startswith('$'):\n",
        "              currency = 'USD'\n",
        "              value = value.replace('$', '')\n",
        "              multiplier = 83 \n",
        "          elif value.startswith('₹'):\n",
        "              currency = 'INR'\n",
        "              value = value.replace('₹', '')\n",
        "              multiplier = 1\n",
        "          elif value.startswith('€') or value.startswith('£'):\n",
        "              return value \n",
        "\n",
        "          value = value.replace(',', '')\n",
        "\n",
        "          try:\n",
        "              num = float(re.findall(r'\\d+(?:\\.\\d+)?', value)[0])\n",
        "          except (IndexError, ValueError):\n",
        "              return \"\"\n",
        "\n",
        "          num_in_inr = int(num * multiplier)\n",
        "\n",
        "          return f\"₹{num_in_inr:,}\"\n",
        "\n",
        "        # Budget - from box office section\n",
        "        try:\n",
        "            budget_element = driver.find_element(By.CSS_SELECTOR, 'li[data-testid=\"title-boxoffice-budget\"] .ipc-metadata-list-item__list-content-item')\n",
        "            budget = budget_element.text.strip()\n",
        "        except:\n",
        "            budget = \"\"\n",
        "\n",
        "        details[\"budget\"] = clean_money_value(budget)\n",
        "\n",
        "\n",
        "        # Worldwide gross (collection)\n",
        "        try:\n",
        "            collection_element = driver.find_element(By.CSS_SELECTOR, 'li[data-testid=\"title-boxoffice-cumulativeworldwidegross\"] .ipc-metadata-list-item__list-content-item')\n",
        "            collection = collection_element.text.strip()\n",
        "        except:\n",
        "            collection = \"\"\n",
        "\n",
        "        details[\"collection\"] = clean_money_value(collection)\n",
        "\n",
        "\n",
        "        # Extract Director(s)\n",
        "        try:\n",
        "            elements = driver.find_elements(By.XPATH, '//li[.//span[text()=\"Director\"]]//a[contains(@href, \"/name/\")]')\n",
        "            seen = set()\n",
        "            directors = []\n",
        "            for elem in elements:\n",
        "                name = elem.text.strip()\n",
        "                if name and name not in seen:\n",
        "                    seen.add(name)\n",
        "                    directors.append(name)\n",
        "        except:\n",
        "            directors = []\n",
        "\n",
        "        details[\"directors\"] = \", \".join(directors)\n",
        "\n",
        "\n",
        "\n",
        "        # Calculate popularity\n",
        "        try:\n",
        "            vote_avg = float(details[\"vote_average\"])\n",
        "            vote_count = int(details[\"vote_count\"].replace(\",\", \"\"))\n",
        "            details[\"popularity\"] = str(round(vote_avg * (vote_count / 1000), 2))\n",
        "        except:\n",
        "            details[\"popularity\"] = \"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting details for {imdb_id}: {e}\")\n",
        "\n",
        "\n",
        "    # Sleep to avoid overloading the server\n",
        "    time.sleep(random.uniform(2, 4))\n",
        "\n",
        "    return details\n",
        "\n",
        "def get_completed_ids(output_file):\n",
        "    \"\"\"Get list of imdb_ids that have already been processed\"\"\"\n",
        "    if not os.path.exists(output_file):\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(output_file)\n",
        "        return set(df['imdb_id'].tolist())\n",
        "    except:\n",
        "        return set()\n",
        "\n",
        "def main(input_file, output_file):\n",
        "    \"\"\"Main function to run the scraper\"\"\"\n",
        "\n",
        "    !apt-get update\n",
        "    !apt install -y chromium-browser\n",
        "    !apt-get install -y chromium-driver\n",
        "\n",
        "\n",
        "    try:\n",
        "        df_input = pd.read_csv(input_file)\n",
        "        print(f\"Loaded input file with {len(df_input)} entries\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading input file: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Check if output file exists, and get already completed IDs\n",
        "    completed_ids = get_completed_ids(output_file)\n",
        "    print(f\"Found {len(completed_ids)} already processed entries\")\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        df_output = pd.read_csv(output_file)\n",
        "    else:\n",
        "        df_output = pd.DataFrame(columns=COLUMNS)\n",
        "\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
        "    chrome_options.add_argument('--disable-extensions')\n",
        "\n",
        "    # Create a new Chrome driver\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "    driver.set_page_load_timeout(30)\n",
        "\n",
        "    try:\n",
        "        for index, row in df_input.iterrows():\n",
        "            imdb_id = row['imdb_id']\n",
        "\n",
        "            if imdb_id in completed_ids:\n",
        "                print(f\"Skipping {imdb_id} - already processed\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing {index+1}/{len(df_input)}: {imdb_id}\")\n",
        "\n",
        "            details = get_title_details(imdb_id, driver)\n",
        "            df_output = pd.concat([df_output, pd.DataFrame([details])], ignore_index=True)\n",
        "            df_output.to_csv(output_file, index=False)\n",
        "            print(f\"Saved progress for {imdb_id}\")\n",
        "\n",
        "            # Add random pause between requests\n",
        "            pause_time = random.uniform(*DELAY_BETWEEN_REQUESTS)\n",
        "            print(f\"Pausing for {pause_time:.2f} seconds...\")\n",
        "            time.sleep(pause_time)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Scraping interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during scraping: {e}\")\n",
        "    finally:\n",
        "\n",
        "        df_output.to_csv(output_file, index=False)\n",
        "        print(f\"Saved results to {output_file}\")\n",
        "\n",
        "        driver.quit()\n",
        "        print(\"Driver closed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    input_file = \"/content/drive/MyDrive/indian_tv_shows.csv\"\n",
        "    output_file = \"/content/drive/MyDrive/tv_imdb_details.csv\"\n",
        "\n",
        "    main(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_plot_synopsis(imdb_id):\n",
        "    url = f\"https://www.imdb.com/title/{imdb_id}/plotsummary/?ref_=tt_stry_pl#synopsis\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Look for the synopsis section\n",
        "    synopsis_container = soup.find('div', {'data-testid': 'sub-section-synopsis'})\n",
        "    if synopsis_container:\n",
        "        synopsis_div = synopsis_container.find('div', class_='ipc-html-content-inner-div')\n",
        "        if synopsis_div:\n",
        "            synopsis_text = synopsis_div.get_text(separator=' ', strip=True)\n",
        "            return synopsis_text\n",
        "\n",
        "    return None\n",
        "\n",
        "# Example usage\n",
        "imdb_id = \"tt15239678\"\n",
        "synopsis = get_plot_synopsis(imdb_id)\n",
        "print(synopsis[:500])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ5rrKf1rFaW",
        "outputId": "7a4ead10-e1d5-482b-aed3-a06367488a91"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import concurrent.futures\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "# Constants\n",
        "BASE_URL = \"https://www.imdb.com\"\n",
        "COLUMNS = [\n",
        "    \"imdb_id\", \"title\", \"vote_average\", \"vote_count\", \"release_date\",\n",
        "    \"original_language\", \"overview\", \"popularity\", \"genres\",\n",
        "    \"production_companies\", \"release_year\", \"cast\",\n",
        "    \"budget\", \"collection\", \"directors\"\n",
        "]\n",
        "NUM_WORKERS = 8  \n",
        "BATCH_SIZE = 20\n",
        "MAX_RETRIES = 5\n",
        "DELAY_BETWEEN_REQUESTS = (0.2, 1.0)  \n",
        "\n",
        "# Setup session with retries\n",
        "def create_session():\n",
        "    session = requests.Session()\n",
        "\n",
        "    retries = Retry(\n",
        "        total=MAX_RETRIES,\n",
        "        backoff_factor=0.5,  \n",
        "        status_forcelist=[429, 500, 502, 503, 504],\n",
        "        allowed_methods=[\"GET\"]\n",
        "    )\n",
        "\n",
        "    adapter = HTTPAdapter(max_retries=retries)\n",
        "    session.mount(\"http://\", adapter)\n",
        "    session.mount(\"https://\", adapter)\n",
        "\n",
        "    session.timeout = 20\n",
        "\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36 Edg/119.0.0.0',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (iPad; CPU OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1'\n",
        "    ]\n",
        "    session.headers.update({\n",
        "        'User-Agent': random.choice(user_agents),\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5',\n",
        "        'DNT': '1',\n",
        "        'Connection': 'keep-alive',\n",
        "        'Upgrade-Insecure-Requests': '1',\n",
        "    })\n",
        "\n",
        "    return session\n",
        "\n",
        "def get_title_details(imdb_id, session):\n",
        "    \"\"\"Scrape plot summary for a specific title using requests\"\"\"\n",
        "    details = {col: \"\" for col in COLUMNS}\n",
        "    details[\"imdb_id\"] = imdb_id\n",
        "\n",
        "    title_url = f\"{BASE_URL}/title/{imdb_id}/\"\n",
        "\n",
        "    try:\n",
        "        response = session.get(title_url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error fetching {imdb_id}: HTTP status {response.status_code}\")\n",
        "            return details\n",
        "\n",
        "        # Parse the HTML with BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "\n",
        "        title = None\n",
        "        title_elem = soup.select_one('h1[data-testid=\"hero-title-block__title\"]')\n",
        "        if not title_elem:\n",
        "            title_elem = soup.select_one('h1.hero__title')\n",
        "        if title_elem:\n",
        "            title = title_elem.text.strip()\n",
        "\n",
        "        # If title not found, look for it in page title\n",
        "        if not title:\n",
        "            page_title = soup.title.text if soup.title else \"\"\n",
        "            title_match = re.match(r'(.+?)(?:\\s*\\([0-9]{4}\\))?\\s*-\\s*IMDb', page_title)\n",
        "            if title_match:\n",
        "                title = title_match.group(1).strip()\n",
        "\n",
        "        details[\"title\"] = title or \"\"\n",
        "\n",
        "        # Rating (vote_average)\n",
        "        rating_elem = soup.select_one('div[data-testid=\"hero-rating-bar__aggregate-rating__score\"] span:first-child')\n",
        "        if not rating_elem:\n",
        "            rating_elem = soup.select_one('span.AggregateRatingButton__RatingScore-')\n",
        "\n",
        "        if rating_elem:\n",
        "            rating = rating_elem.text.strip().replace('/10', '')\n",
        "            try:\n",
        "                float(rating)\n",
        "                details[\"vote_average\"] = rating\n",
        "            except:\n",
        "                details[\"vote_average\"] = \"\"\n",
        "\n",
        "       # Vote count\n",
        "        vote_count_selectors = [\n",
        "            'div[class*=\"AggregateRatingButton__TotalRatingAmount\"]',\n",
        "            'div.sc-d541859f-3',\n",
        "            'div.imdbRating strong',\n",
        "            'span.ipc-rating-star--imdb + div',\n",
        "            'a[href*=\"ratings\"] span'\n",
        "        ]\n",
        "\n",
        "        vote_count = \"\"\n",
        "        for selector in vote_count_selectors:\n",
        "            el = soup.select_one(selector)\n",
        "            if el and el.get_text(strip=True):\n",
        "                vote_count = el.get_text(strip=True)\n",
        "                break\n",
        "\n",
        "\n",
        "        def extract_numeric_value(text):\n",
        "            if not text:\n",
        "                return \"\"\n",
        "            match = re.search(r'([\\d,.]+)\\s*([KM]?)', text, re.IGNORECASE)\n",
        "            if not match:\n",
        "                return \"\"\n",
        "\n",
        "            number, suffix = match.groups()\n",
        "            number = number.replace(',', '').strip()\n",
        "\n",
        "            try:\n",
        "                num = float(number)\n",
        "                suffix = suffix.upper()\n",
        "                multiplier = {\n",
        "                    \"\": 1,\n",
        "                    \"K\": 1_000,\n",
        "                    \"M\": 1_000_000,\n",
        "                }.get(suffix, 1)\n",
        "                return str(int(num * multiplier))\n",
        "            except:\n",
        "                return \"\"\n",
        "\n",
        "\n",
        "        vote_count_clean = extract_numeric_value(vote_count)\n",
        "        details[\"vote_count\"] = vote_count_clean\n",
        "\n",
        "        # Fallback: check raw text for \"votes\"\n",
        "        if not details[\"vote_count\"]:\n",
        "            text_elements = soup.find_all(string=re.compile(r'\\d[\\d,]*\\s+votes'))\n",
        "            for txt in text_elements:\n",
        "                votes_match = re.search(r'([\\d,]+)\\s*votes', txt)\n",
        "                if votes_match:\n",
        "                    details[\"vote_count\"] = votes_match.group(1).replace(',', '')\n",
        "                    break\n",
        "\n",
        "        # Release date\n",
        "        release_date_elem = soup.select_one('li[data-testid=\"title-details-releasedate\"] .ipc-metadata-list-item__list-content-item')\n",
        "        if not release_date_elem:\n",
        "            release_date_elem = soup.select_one('a[href*=\"releaseinfo\"]')\n",
        "\n",
        "        if release_date_elem:\n",
        "            release_date = release_date_elem.text.strip()\n",
        "            details[\"release_date\"] = release_date\n",
        "\n",
        "            # Release year\n",
        "            year_match = re.search(r'(\\d{4})', release_date)\n",
        "            if year_match:\n",
        "                details[\"release_year\"] = year_match.group(1)\n",
        "\n",
        "        # If year not found in release date, try other selectors\n",
        "        if not details[\"release_year\"]:\n",
        "            year_elem = soup.select_one('span.TitleBlockMetaData__ReleaseYear-')\n",
        "            if year_elem:\n",
        "                year_match = re.search(r'(\\d{4})', year_elem.text)\n",
        "                if year_match:\n",
        "                    details[\"release_year\"] = year_match.group(1)\n",
        "                    \n",
        "            if not details[\"release_year\"]:\n",
        "                page_title = soup.title.text if soup.title else \"\"\n",
        "                year_match = re.search(r'\\((\\d{4})\\)', page_title)\n",
        "                if year_match:\n",
        "                    details[\"release_year\"] = year_match.group(1)\n",
        "\n",
        "        # Overview/Plot\n",
        "        plot_elem = soup.select_one('span[class*=\"GenresAndPlot__TextContainerBreakpointXL\"]')\n",
        "        if not plot_elem:\n",
        "            plot_elem = soup.select_one('p[data-testid=\"plot\"]')\n",
        "\n",
        "        if plot_elem:\n",
        "            details[\"overview\"] = plot_elem.text.strip()\n",
        "\n",
        "        # Genres\n",
        "        genre_elems = soup.select('div[data-testid=\"genres\"] span.ipc-chip__text')\n",
        "        if not genre_elems:\n",
        "            genre_elems = soup.select('.ipc-chip-list--baseAlt span.ipc-chip__text')\n",
        "\n",
        "        genres = []\n",
        "        for elem in genre_elems:\n",
        "            text = elem.text.strip()\n",
        "            if text and \"more\" not in text.lower():\n",
        "                genres.append(text)\n",
        "\n",
        "        details[\"genres\"] = \", \".join(genres)\n",
        "\n",
        "        # Language\n",
        "        lang_elem = soup.select_one('li[data-testid=\"title-details-languages\"] .ipc-metadata-list-item__list-content-item')\n",
        "        if lang_elem:\n",
        "            details[\"original_language\"] = lang_elem.text.strip()\n",
        "\n",
        "        # Production companies\n",
        "        company_elems = soup.select('li[data-testid=\"title-details-companies\"] .ipc-metadata-list-item__list-content-item')\n",
        "        companies = [elem.text.strip() for elem in company_elems if elem.text.strip()]\n",
        "        details[\"production_companies\"] = \", \".join(companies)\n",
        "\n",
        "        # Cast\n",
        "        cast_elems = soup.select('a[data-testid=\"title-cast-item__actor\"]')\n",
        "        if not cast_elems:\n",
        "            cast_elems = soup.select('td.primary_photo + td a')\n",
        "\n",
        "        cast = [elem.text.strip() for elem in cast_elems[:10] if elem.text.strip()]\n",
        "        details[\"cast\"] = \", \".join(cast)\n",
        "\n",
        "        def clean_money_value(value):\n",
        "            if not value:\n",
        "                return \"\"\n",
        "            value = value.strip()\n",
        "            value = re.sub(r'\\(.*?\\)', '', value).strip()\n",
        "            multiplier = 1\n",
        "            currency = \"\"\n",
        "\n",
        "            if value.startswith('$'):\n",
        "                currency = 'USD'\n",
        "                value = value.replace('$', '')\n",
        "                multiplier = 83 \n",
        "            elif value.startswith('₹'):\n",
        "                currency = 'INR'\n",
        "                value = value.replace('₹', '')\n",
        "                multiplier = 1\n",
        "            elif value.startswith('€') or value.startswith('£'):\n",
        "                return value \n",
        "\n",
        "            value = value.replace(',', '')\n",
        "            try:\n",
        "                num = float(re.findall(r'\\d+(?:\\.\\d+)?', value)[0])\n",
        "            except (IndexError, ValueError):\n",
        "                return \"\"\n",
        "\n",
        "            num_in_inr = int(num * multiplier)\n",
        "            return f\"₹{num_in_inr:,}\"\n",
        "\n",
        "        # Budget - from box office section\n",
        "        budget_elem = soup.select_one('li[data-testid=\"title-boxoffice-budget\"] .ipc-metadata-list-item__list-content-item')\n",
        "        budget = budget_elem.text.strip() if budget_elem else \"\"\n",
        "        details[\"budget\"] = clean_money_value(budget)\n",
        "\n",
        "        # Worldwide gross (collection)\n",
        "        collection_elem = soup.select_one('li[data-testid=\"title-boxoffice-cumulativeworldwidegross\"] .ipc-metadata-list-item__list-content-item')\n",
        "        collection = collection_elem.text.strip() if collection_elem else \"\"\n",
        "        details[\"collection\"] = clean_money_value(collection)\n",
        "\n",
        "        # Extract Director(s)\n",
        "        directors = []\n",
        "        director_section = soup.find(string=re.compile(\"Director\"))\n",
        "        if director_section and director_section.parent:\n",
        "            director_li = director_section.find_parent('li')\n",
        "            if director_li:\n",
        "                director_links = director_li.select('a[href*=\"/name/\"]')\n",
        "                seen = set()\n",
        "                for link in director_links:\n",
        "                    name = link.text.strip()\n",
        "                    if name and name not in seen:\n",
        "                        seen.add(name)\n",
        "                        directors.append(name)\n",
        "\n",
        "        details[\"directors\"] = \", \".join(directors)\n",
        "        \n",
        "        # Runtime\n",
        "        runtime_elem = soup.select_one('li[data-testid=\"title-techspec_runtime\"]')\n",
        "        if runtime_elem:\n",
        "            runtime_text = runtime_elem.get_text(separator=\" \", strip=True)\n",
        "            details[\"runtime\"] = runtime_text.replace(\"Runtime\", \"\").strip()\n",
        "        else:\n",
        "            details[\"runtime\"] = None\n",
        "\n",
        "       # Country of origin\n",
        "        origin_elem = soup.select_one('li[data-testid=\"title-details-origin\"]')\n",
        "        if origin_elem:\n",
        "            country_links = origin_elem.select('a.ipc-metadata-list-item__list-content-item')\n",
        "            countries = [a.get_text(strip=True) for a in country_links if a.get_text(strip=True)]\n",
        "            details[\"country_of_origin\"] = \", \".join(countries)\n",
        "        else:\n",
        "            details[\"country_of_origin\"] = None\n",
        "\n",
        "\n",
        "        # Cast (Actor: Character)\n",
        "        cast_items = []\n",
        "        cast_blocks = soup.select('div[data-testid=\"title-cast-item\"]')\n",
        "        if cast_blocks:\n",
        "            for block in cast_blocks[:10]:\n",
        "                actor_elem = block.select_one('a[data-testid=\"title-cast-item__actor\"]')\n",
        "                if actor_elem:\n",
        "                    actor_name = actor_elem.get_text(strip=True)\n",
        "\n",
        "                    # Try to get character information\n",
        "                    char_elem = block.select_one('a[data-testid=\"cast-item-characters-link\"] span.sc-cd7dc4b7-4') or \\\n",
        "                              block.select_one('span.sc-cd7dc4b7-4')\n",
        "\n",
        "                    extra_info = block.select_one('span.sc-cd7dc4b7-9')\n",
        "\n",
        "                    if char_elem:\n",
        "                        char_name = char_elem.get_text(strip=True)\n",
        "                        if extra_info:\n",
        "                            cast_items.append(f\"{actor_name}: {char_name} {extra_info.get_text(strip=True)}\")\n",
        "                        else:\n",
        "                            cast_items.append(f\"{actor_name}: {char_name}\")\n",
        "                    else:\n",
        "                        # Add just the actor name if no character info is found\n",
        "                        cast_items.append(actor_name)\n",
        "\n",
        "        # Fallback \n",
        "        if not cast_items:\n",
        "            cast_elems = soup.select('a[data-testid=\"title-cast-item__actor\"]')\n",
        "            if not cast_elems:\n",
        "                cast_elems = soup.select('td.primary_photo + td a')\n",
        "            cast_items = [elem.get_text(strip=True) for elem in cast_elems[:10] if elem.get_text(strip=True)]\n",
        "\n",
        "        details[\"cast\"] = \", \".join(cast_items)\n",
        "\n",
        "        # Calculate popularity\n",
        "        try:\n",
        "            vote_avg = float(details[\"vote_average\"])\n",
        "            vote_count = int(details[\"vote_count\"].replace(\",\", \"\"))\n",
        "            details[\"popularity\"] = str(round(vote_avg * (vote_count / 1000), 2))\n",
        "        except:\n",
        "            details[\"popularity\"] = \"\"\n",
        "\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {imdb_id}: {str(e)[:100]}...\")\n",
        "\n",
        "    time.sleep(random.uniform(*DELAY_BETWEEN_REQUESTS))\n",
        "\n",
        "    return details\n",
        "\n",
        "def process_batch(imdb_ids_batch):\n",
        "    \"\"\"Process a batch of IMDb IDs using a dedicated session\"\"\"\n",
        "    results = []\n",
        "    session = create_session()\n",
        "\n",
        "    for imdb_id in imdb_ids_batch:\n",
        "        try:\n",
        "            details = get_title_details(imdb_id, session)\n",
        "            results.append(details)\n",
        "            print(f\"Processed: {imdb_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {imdb_id}: {str(e)[:100]}...\")\n",
        "            results.append({\"imdb_id\": imdb_id, \"overview\": \"\"})\n",
        "\n",
        "    return results\n",
        "\n",
        "def get_completed_ids(output_file):\n",
        "    \"\"\"Get list of imdb_ids that have already been processed\"\"\"\n",
        "    if not os.path.exists(output_file):\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(output_file)\n",
        "        return set(df['imdb_id'].tolist())\n",
        "    except:\n",
        "        return set()\n",
        "\n",
        "def main(input_file, output_file):\n",
        "    \"\"\"Main function to run the parallel requests-based scraper\"\"\"\n",
        "    try:\n",
        "        df_input = pd.read_csv(input_file)\n",
        "        print(f\"Loaded input file with {len(df_input)} entries\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading input file: {e}\")\n",
        "        raise\n",
        "\n",
        "    completed_ids = get_completed_ids(output_file)\n",
        "    print(f\"Found {len(completed_ids)} already processed entries\")\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        df_output = pd.read_csv(output_file)\n",
        "    else:\n",
        "        df_output = pd.DataFrame(columns=COLUMNS)\n",
        "\n",
        "    ids_to_process = [\n",
        "        row['imdb_id'] for _, row in df_input.iterrows()\n",
        "        if row['imdb_id'] not in completed_ids\n",
        "    ]\n",
        "    print(f\"Processing {len(ids_to_process)} remaining entries\")\n",
        "\n",
        "    batches = []\n",
        "    batch_size = 20  \n",
        "    for i in range(0, len(ids_to_process), batch_size):\n",
        "        batches.append(ids_to_process[i:i + batch_size])\n",
        "\n",
        "    try:\n",
        "        for batch_idx, batch in enumerate(batches):\n",
        "            print(f\"Processing batch {batch_idx+1}/{len(batches)}\")\n",
        "\n",
        "            # Split batch into smaller chunks \n",
        "            sub_batches = []\n",
        "            chunk_size = max(1, len(batch) // NUM_WORKERS)\n",
        "            for i in range(0, len(batch), chunk_size):\n",
        "                sub_batches.append(batch[i:i + chunk_size])\n",
        "\n",
        "            # Process sub-batches in parallel\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
        "                futures = [executor.submit(process_batch, sub_batch) for sub_batch in sub_batches]\n",
        "\n",
        "                batch_results = []\n",
        "                for future in concurrent.futures.as_completed(futures):\n",
        "                    batch_results.extend(future.result())\n",
        "\n",
        "            new_data = pd.DataFrame(batch_results)\n",
        "            df_output = pd.concat([df_output, new_data], ignore_index=True)\n",
        "\n",
        "            # Save progress after each batch\n",
        "            df_output.to_csv(output_file, index=False)\n",
        "            print(f\"Saved progress after batch {batch_idx+1}\")\n",
        "            if batch_idx < len(batches) - 1:\n",
        "                time.sleep(random.uniform(1, 2))\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Scraping interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during scraping: {e}\")\n",
        "    finally:\n",
        "        df_output.to_csv(output_file, index=False)\n",
        "        print(f\"Saved results to {output_file}\")\n",
        "        print(\"Scraping complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/indian_movies_ids.csv\"\n",
        "    output_file = \"/content/om.csv\"\n",
        "\n",
        "    main(input_file, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8S_x0morZ2x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "USER_AGENTS = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
        "]\n",
        "\n",
        "# Constants\n",
        "INPUT_CSV = '/content/plo.csv' \n",
        "OUTPUT_CSV = '/content/infoo.csv'\n",
        "CHECKPOINT_FILE = '/content/progress.json'\n",
        "SAVE_INTERVAL = 100 \n",
        "\n",
        "def clean_title(title):\n",
        "    \"\"\"Clean the title by removing year and special characters if present.\"\"\"\n",
        "    title = re.sub(r'\\s*\\(\\d{4}\\)\\s*', '', title)\n",
        "    title = re.sub(r'[^\\w\\s]', '', title)\n",
        "    return title.strip()\n",
        "\n",
        "def search_wikipedia(title, try_variations=True):\n",
        "    \"\"\"\n",
        "    Search Wikipedia for a movie/TV show and return the URL of the best matching result.\n",
        "    Implements flexible matching to handle various title formats.\n",
        "    \"\"\"\n",
        "    user_agent = random.choice(USER_AGENTS)\n",
        "\n",
        "    # Extract the base title and year if present in format \"Title (year)\"\n",
        "    year = None\n",
        "    base_title = title\n",
        "    year_match = re.search(r'(.+?)\\s*\\((\\d{4}(?:[–\\-]\\d{4}| )?)\\)', title)\n",
        "    if year_match:\n",
        "        base_title = year_match.group(1).strip()\n",
        "        year = year_match.group(2).strip()\n",
        "\n",
        "    # Create search variations in priority order\n",
        "    search_variations = []\n",
        "\n",
        "    # Priority 1: Exact title as provided\n",
        "    search_variations.append(title)\n",
        "\n",
        "    if try_variations:\n",
        "        # Priority 2: If year is present, try \"[Title] [year] film\"\n",
        "        if year:\n",
        "            search_variations.append(f\"{base_title} {year} film\")\n",
        "            search_variations.append(f\"{base_title} {year} movie\")\n",
        "\n",
        "        # Priority 3: Base title with \"film\" or \"movie\" qualifier\n",
        "        search_variations.append(f\"{base_title} film\")\n",
        "        search_variations.append(f\"{base_title} movie\")\n",
        "\n",
        "        # Priority 4: For TV Series, try those variations\n",
        "        search_variations.append(f\"{base_title} TV series\")\n",
        "        search_variations.append(f\"{base_title} television series\")\n",
        "\n",
        "        # Priority 5: Just the base title\n",
        "        search_variations.append(base_title)\n",
        "\n",
        "        # Priority 6: For titles with subtitle (contains colon)\n",
        "        if ':' in base_title:\n",
        "            main_title = base_title.split(':', 1)[0].strip()\n",
        "            search_variations.append(main_title)\n",
        "            if year:\n",
        "                search_variations.append(f\"{main_title} {year} film\")\n",
        "            search_variations.append(f\"{main_title} film\")\n",
        "            search_variations.append(f\"{main_title} movie\")\n",
        "            search_variations.append(f\"{main_title} TV series\")\n",
        "\n",
        "    # Remove duplicates while preserving order\n",
        "    search_variations = list(dict.fromkeys(search_variations))\n",
        "\n",
        "    logger.info(f\"Searching for '{title}' with variations: {search_variations}\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Try each search variation\n",
        "    for search_term in search_variations:\n",
        "        try:\n",
        "            response = requests.get(\n",
        "                'https://en.wikipedia.org/w/api.php',\n",
        "                params={\n",
        "                    'action': 'opensearch',\n",
        "                    'search': search_term,\n",
        "                    'limit': '5', \n",
        "                    'namespace': '0',\n",
        "                    'format': 'json'\n",
        "                },\n",
        "                headers={'User-Agent': user_agent}\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if data and len(data) >= 4 and data[3] and len(data[3]) > 0:\n",
        "                    for i, result_title in enumerate(data[1]):\n",
        "                        all_results.append({\n",
        "                            'title': result_title,\n",
        "                            'url': data[3][i],\n",
        "                            'search_term': search_term\n",
        "                        })\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error searching for '{search_term}': {str(e)}\")\n",
        "\n",
        "    if not all_results:\n",
        "        return None\n",
        "\n",
        "    # Score and rank results\n",
        "    scored_results = score_search_results(all_results, base_title, year)\n",
        "\n",
        "    # Return the URL of the highest scoring result\n",
        "    if scored_results:\n",
        "        best_match = scored_results[0]\n",
        "        logger.info(f\"Best match for '{title}': '{best_match['title']}' (Score: {best_match['score']})\")\n",
        "        return best_match['url']\n",
        "\n",
        "    return None\n",
        "\n",
        "def score_search_results(results, base_title, year=None):\n",
        "    \"\"\"\n",
        "    Score search results based on how well they match the original title and year.\n",
        "\n",
        "    Args:\n",
        "        results: List of dict with 'title' and 'url' keys\n",
        "        base_title: Original title without year\n",
        "        year: Year of release if available\n",
        "\n",
        "    Returns:\n",
        "        List of results sorted by score (highest first)\n",
        "    \"\"\"\n",
        "    scored_results = []\n",
        "    base_title_lower = base_title.lower()\n",
        "\n",
        "    for result in results:\n",
        "        result_title = result['title']\n",
        "        result_lower = result_title.lower()\n",
        "        score = 0\n",
        "\n",
        "        # Exact match is best\n",
        "        if result_lower == base_title_lower:\n",
        "            score += 100\n",
        "\n",
        "        elif result_lower.startswith(base_title_lower):\n",
        "            score += 80\n",
        "\n",
        "        elif base_title_lower in result_lower:\n",
        "            score += 60\n",
        "\n",
        "        if 'film' in result_lower or 'movie' in result_lower:\n",
        "            score += 30\n",
        "        elif 'tv series' in result_lower or 'television series' in result_lower:\n",
        "            score += 25\n",
        "\n",
        "        # Check for year match if we have a year\n",
        "        if year:\n",
        "            if year in result_title:\n",
        "                score += 40\n",
        "            elif re.search(r'\\b\\d{4}\\b', result_title):\n",
        "                score += 10\n",
        "\n",
        "        # Is it specifically marked as a film or TV show?\n",
        "        if re.search(r'\\(\\d{4}.*(?:film|movie)\\)', result_title):\n",
        "            score += 15\n",
        "        elif re.search(r'\\((?:TV|television) series\\)', result_title):\n",
        "            score += 15\n",
        "\n",
        "        # Avoid disambiguation pages\n",
        "        if '(disambiguation)' in result_lower:\n",
        "            score -= 100\n",
        "\n",
        "        if 'actor' in result_lower or 'actress' in result_lower or 'director' in result_lower:\n",
        "            score -= 50\n",
        "\n",
        "        scored_result = result.copy()\n",
        "        scored_result['score'] = score\n",
        "        scored_results.append(scored_result)\n",
        "\n",
        "    return sorted(scored_results, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "def extract_movie_info(url):\n",
        "    \"\"\"Extract franchise and other info from a Wikipedia page.\"\"\"\n",
        "    if not url:\n",
        "        return \" \"\n",
        "\n",
        "    user_agent = random.choice(USER_AGENTS)\n",
        "    additional_info = []\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': user_agent})\n",
        "        if response.status_code != 200:\n",
        "            return f\"Failed to access Wikipedia page: {response.status_code}\", \"\"\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        infobox = soup.find('table', {'class': 'infobox'})\n",
        "        if not infobox:\n",
        "            return \" \"\n",
        "\n",
        "        franchise_info = extract_franchise_info(soup, infobox)\n",
        "        if franchise_info:\n",
        "            additional_info.append(franchise_info)\n",
        "\n",
        "        sequel_info = extract_sequel_info(soup)\n",
        "        if sequel_info:\n",
        "            additional_info.append(sequel_info)\n",
        "\n",
        "        universe_info = extract_universe_info(soup, infobox)\n",
        "        if universe_info:\n",
        "            additional_info.append(universe_info)\n",
        "\n",
        "        if not additional_info:\n",
        "            return \" \"\n",
        "\n",
        "        return \"; \".join(additional_info)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting info from {url}: {str(e)}\")\n",
        "        return f\"Error: {str(e)}\", \"\"\n",
        "\n",
        "\n",
        "def extract_franchise_info(soup, infobox):\n",
        "    \"\"\"Extract franchise/series information.\"\"\"\n",
        "    # Method 1: Check infobox for series info\n",
        "    for row in infobox.find_all('tr'):\n",
        "        header = row.find('th')\n",
        "        if header and header.text and ('series' in header.text.lower() or 'franchise' in header.text.lower()):\n",
        "            value = row.find('td')\n",
        "            if value:\n",
        "                return f\"{value.text.strip()}\"\n",
        "\n",
        "    # Method 2: Look for series categories\n",
        "    categories = soup.find('div', {'id': 'mw-normal-catlinks'})\n",
        "    if categories:\n",
        "        category_links = categories.find_all('a')\n",
        "        for link in category_links:\n",
        "            text = link.text.lower()\n",
        "            if 'film series' in text or 'franchise' in text:\n",
        "                return f\"{link.text}\"\n",
        "\n",
        "    # Method 3: Look for \"Part of\" text in the first few paragraphs\n",
        "    content = soup.find('div', {'id': 'mw-content-text'})\n",
        "    if content:\n",
        "        paragraphs = content.find_all('p', limit=5)\n",
        "        for p in paragraphs:\n",
        "            text = p.text.lower()\n",
        "            franchise_patterns = [\n",
        "                r'part of (?:the )?(.*?(?:franchise|series|trilogy|universe))',\n",
        "                r'(?:is|was) (?:the|a) ((?:\\w+\\s)+(?:film series|franchise|trilogy))',\n",
        "                r'(?:is|was) (?:the|a) ((?:\\w+\\s)+(?:installment|film)) in the (.*?(?:series|franchise|trilogy))'\n",
        "            ]\n",
        "\n",
        "            for pattern in franchise_patterns:\n",
        "                match = re.search(pattern, text, re.IGNORECASE)\n",
        "                if match:\n",
        "                    if len(match.groups()) > 1 and match.group(2):\n",
        "                        return f\"{match.group(2).strip()}\"\n",
        "                    return f\"{match.group(1).strip()}\"\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_sequel_info(soup):\n",
        "    \"\"\"Extract sequel/prequel information.\"\"\"\n",
        "    content = soup.find('div', {'id': 'mw-content-text'})\n",
        "    if not content:\n",
        "        return None\n",
        "\n",
        "    # Search for sequel/prequel mentions in the first few sections\n",
        "    sequel_info = []\n",
        "\n",
        "    # Check for \"See also\" or \"Other films\" sections\n",
        "    see_also = soup.find('span', {'id': re.compile('See_also|Other_films')})\n",
        "    if see_also:\n",
        "        see_also_section = see_also.parent.find_next('ul')\n",
        "        if see_also_section:\n",
        "            for li in see_also_section.find_all('li'):\n",
        "                if 'sequel' in li.text.lower() or 'prequel' in li.text.lower() or 'follow' in li.text.lower():\n",
        "                    sequel_info.append(li.text.strip())\n",
        "\n",
        "    # Check the first few paragraphs\n",
        "    paragraphs = content.find_all('p', limit=10)\n",
        "    for p in paragraphs:\n",
        "        text = p.text.lower()\n",
        "        if 'sequel' in text or 'prequel' in text or 'preceded by' in text:\n",
        "            # Extract the sequel/prequel information\n",
        "            sequel_patterns = [\n",
        "                r'(?:sequel to|follow-up)[^\\.\\,]*((?:titled |called |named )?[\\w\\s:]+)',\n",
        "                r'(?:prequel|preceded by)[^\\.\\,]*((?:titled |called |named )?[\\w\\s:]+)'\n",
        "            ]\n",
        "\n",
        "            for pattern in sequel_patterns:\n",
        "                match = re.search(pattern, text, re.IGNORECASE)\n",
        "                if match:\n",
        "                    sequel_text = match.group(0).strip()\n",
        "                    sequel_text = re.sub(r'\\[\\d+\\]', '', sequel_text)\n",
        "                    sequel_info.append(sequel_text)\n",
        "\n",
        "    if sequel_info:\n",
        "        return f\"{'; '.join(sequel_info)}\"\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_universe_info(soup, infobox):\n",
        "    \"\"\"Extract universe information (MCU, DCEU, etc.).\"\"\"\n",
        "    # Search for universe mentions in the infobox\n",
        "    for row in infobox.find_all('tr'):\n",
        "        header = row.find('th')\n",
        "        if header and header.text and 'universe' in header.text.lower():\n",
        "            value = row.find('td')\n",
        "            if value:\n",
        "                return f\"{value.text.strip()}\"\n",
        "\n",
        "    # Check the first few paragraphs for universe mentions\n",
        "    content = soup.find('div', {'id': 'mw-content-text'})\n",
        "    if content:\n",
        "        paragraphs = content.find_all('p', limit=5)\n",
        "        for p in paragraphs:\n",
        "            text = p.text.lower()\n",
        "            universe_patterns = [\n",
        "                r'(marvel cinematic universe)',\n",
        "                r'(dc extended universe)',\n",
        "                r'(dc universe)',\n",
        "                r'(\\w+ universe)',\n",
        "                r'(star wars universe)',\n",
        "                r'(wizarding world)'\n",
        "            ]\n",
        "\n",
        "            for pattern in universe_patterns:\n",
        "                match = re.search(pattern, text, re.IGNORECASE)\n",
        "                if match:\n",
        "                    return f\"{match.group(1)}\"\n",
        "\n",
        "    # Check categories\n",
        "    categories = soup.find('div', {'id': 'mw-normal-catlinks'})\n",
        "    if categories:\n",
        "        category_links = categories.find_all('a')\n",
        "        for link in category_links:\n",
        "            text = link.text.lower()\n",
        "            if 'universe' in text:\n",
        "                return f\"{link.text}\"\n",
        "    return None\n",
        "\n",
        "def save_checkpoint(processed_indices, results):\n",
        "    \"\"\"Save checkpoint data to file.\"\"\"\n",
        "    checkpoint_data = {\n",
        "        'processed_indices': processed_indices,\n",
        "        'results': results,\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    with open(CHECKPOINT_FILE, 'w') as f:\n",
        "        json.dump(checkpoint_data, f)\n",
        "\n",
        "    logger.info(f\"Checkpoint saved: {len(processed_indices)} movies processed\")\n",
        "\n",
        "def load_checkpoint():\n",
        "    \"\"\"Load checkpoint data if exists.\"\"\"\n",
        "    if not os.path.exists(CHECKPOINT_FILE):\n",
        "        return [], {}\n",
        "\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, 'r') as f:\n",
        "            checkpoint_data = json.load(f)\n",
        "        logger.info(f\"Checkpoint loaded from {checkpoint_data['timestamp']}\")\n",
        "        logger.info(f\"Resuming from {len(checkpoint_data['processed_indices'])} processed movies\")\n",
        "        return checkpoint_data['processed_indices'], checkpoint_data['results']\n",
        "    \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading checkpoint: {str(e)}\")\n",
        "        return [], {}\n",
        "\n",
        "def save_results_to_csv(df, results):\n",
        "    \"\"\"Save current results to CSV file.\"\"\"\n",
        "\n",
        "    df_copy = df.copy()\n",
        "    for idx, info in results.items():\n",
        "        df_copy.loc[int(idx), 'additional_info'] = info\n",
        "\n",
        "    df_copy.to_csv(OUTPUT_CSV, index=False)\n",
        "    logger.info(f\"Updated results saved to {OUTPUT_CSV}\")\n",
        "\n",
        "def process_movie(args):\n",
        "    \"\"\"Process a single movie row.\"\"\"\n",
        "    idx, movie_row = args\n",
        "    title = movie_row['title']\n",
        "\n",
        "    try:\n",
        "        wiki_url = search_wikipedia(title, try_variations=True)\n",
        "\n",
        "        if wiki_url:\n",
        "            time.sleep(random.uniform(0.5, 2))\n",
        "            additional_info = extract_movie_info(wiki_url)\n",
        "            return idx, additional_info\n",
        "        else:\n",
        "            return idx, \" \"\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {title}: {str(e)}\")\n",
        "        return idx, f\"Error: {str(e)}\", \"\"\n",
        "\n",
        "def main():\n",
        "\n",
        "    logger.info(f\"Loading CSV file: {INPUT_CSV}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading CSV: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    if 'additional_info' not in df.columns:\n",
        "        df['additional_info'] = None\n",
        "\n",
        "    processed_indices, results = load_checkpoint()\n",
        "\n",
        "    movie_rows = df.to_dict('records')\n",
        "    to_process = [(idx, row) for idx, row in enumerate(movie_rows) if str(idx) not in processed_indices]\n",
        "\n",
        "    if not to_process:\n",
        "        logger.info(\"All movies have been processed already!\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Starting to process {len(to_process)} remaining movies...\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor: \n",
        "\n",
        "        pbar = tqdm(total=len(df), desc=\"Scraping Wikipedia\")\n",
        "        pbar.update(len(processed_indices))\n",
        "\n",
        "        batch_size = min(SAVE_INTERVAL, len(to_process))\n",
        "        for i in range(0, len(to_process), batch_size):\n",
        "            batch = to_process[i:i+batch_size]\n",
        "            batch_results = list(executor.map(process_movie, batch))\n",
        "\n",
        "            # Update results and processed indices\n",
        "            for idx, info in batch_results:\n",
        "                results[str(idx)] = info\n",
        "\n",
        "                processed_indices.append(str(idx))\n",
        "\n",
        "            pbar.update(len(batch))\n",
        "            save_checkpoint(processed_indices, results)\n",
        "            save_results_to_csv(df, results)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "\n",
        "    save_results_to_csv(df, results)\n",
        "    if os.path.exists(CHECKPOINT_FILE):\n",
        "        os.remove(CHECKPOINT_FILE)\n",
        "\n",
        "    logger.info(\"Processing complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
